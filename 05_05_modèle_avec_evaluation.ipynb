{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuIBYnpDyUMG2p62SvUqWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olfabre/amsProjetSemestre2/blob/main/05_05_mod%C3%A8le_avec_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zN7JrQNuq4k",
        "outputId": "49f1a630-b3d7-4869-a46a-05e4f3194108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Entra√Ænement sur le dataset de mots de passe...\n",
            "[0m 25s (1000 33%) 3.3694]\n",
            "[0m 52s (2000 66%) 2.8599]\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import time, math\n",
        "import matplotlib.pyplot as plt\n",
        "from os import path, makedirs\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "all_characters = string.ascii_letters + string.digits + string.punctuation\n",
        "end_char = '&'\n",
        "selected = string.ascii_letters + string.digits + end_char\n",
        "n_characters = len(all_characters)\n",
        "n_characters_selected = len(selected)\n",
        "\n",
        "chunk_len = 12\n",
        "\n",
        "hidden_size = 256\n",
        "n_layers = 2\n",
        "lr = 0.005\n",
        "n_epochs = 3000\n",
        "\n",
        "\n",
        "train_file = unidecode.unidecode(open(\"sample_data/train2_with_&.txt\").read())\n",
        "train_file_len = len(train_file)\n",
        "val_file = unidecode.unidecode(open(\"sample_data/validation2.txt\").read())\n",
        "val_file_len = len(val_file)\n",
        "\n",
        "def random_chunk(file, file_len):\n",
        "  start_index = random.randint(0, file_len - chunk_len - 1)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "    if string[c] not in selected:\n",
        "      continue\n",
        "    tensor[c] = selected.index(string[c])\n",
        "  return tensor.to(device)\n",
        "\n",
        "def random_training_set(file, file_len):\n",
        "  chunk = random_chunk(file, file_len)\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "def train(inp, target):\n",
        "  hidden = decoder.init_hidden()\n",
        "  decoder.zero_grad()\n",
        "  loss = 0\n",
        "  seq_len = inp.size(0) # R√©cup√©rer la longueur du batch actuel\n",
        "  for c in range(seq_len):\n",
        "    output, hidden = decoder(inp[c], hidden)\n",
        "    loss += criterion(output, target[c].unsqueeze(0))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  train_losses.append(loss.item() / seq_len)\n",
        "  return loss.item() / seq_len\n",
        "\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=25, temperature=0.8):\n",
        "  if predict_len is None:\n",
        "    predict_len = random.randint(5, 12)\n",
        "\n",
        "  hidden = decoder.init_hidden()\n",
        "  prime_input = char_tensor(prime_str)\n",
        "  predicted = prime_str\n",
        "\n",
        "  for p in range(len(prime_str) - 1):\n",
        "    _, hidden = decoder(prime_input[p], hidden)\n",
        "  inp = prime_input[-1]\n",
        "\n",
        "  for _ in range(predict_len):\n",
        "    output, hidden = decoder(inp, hidden)\n",
        "    output_dist = torch.softmax(output.data.view(-1) / temperature, dim=0)\n",
        "    top_i = torch.multinomial(output_dist[:len(selected)], 1)[0]\n",
        "    predicted_char = selected[top_i]\n",
        "\n",
        "    if predicted_char == '&':\n",
        "      break\n",
        "\n",
        "    predicted += predicted_char\n",
        "    inp = torch.tensor([selected.index(predicted_char)]).to(device)\n",
        "\n",
        "  return predicted\n",
        "\n",
        "\n",
        "def time_since(since):\n",
        "  s = time.time() - since\n",
        "  m = math.floor(s / 60)\n",
        "  s -= m * 60\n",
        "  return '%dm %ds' % (m, s)\n",
        "\n",
        "def exponential_moving_average(values, alpha=0.01):\n",
        "  ema = []\n",
        "  avg = values[0] # Initialisation\n",
        "  for value in values:\n",
        "    avg = alpha * value + (1 - alpha) * avg\n",
        "    ema.append(avg)\n",
        "  return ema\n",
        "\n",
        "def evaluate_loss(n_samples=1000):\n",
        "  total_loss = 0\n",
        "  for _ in range(n_samples):\n",
        "    inp, target = random_training_set(val_file, val_file_len)\n",
        "    hidden = decoder.init_hidden()\n",
        "    loss = 0\n",
        "  for c in range(inp.size(0)):\n",
        "    output, hidden = decoder(inp[c], hidden)\n",
        "  loss += criterion(output, target[c].unsqueeze(0))\n",
        "  total_loss += loss.item() / inp.size(0)\n",
        "  return total_loss / n_samples\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers).to(device)\n",
        "optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "print(\"Entra√Ænement sur le dataset de mots de passe...\")\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss = train(*random_training_set(train_file, train_file_len))\n",
        "  with torch.no_grad():\n",
        "    eval_loss = evaluate_loss(n_samples=50)\n",
        "    eval_losses.append(eval_loss)\n",
        "  if epoch % 1000 == 0:\n",
        "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "\n",
        "smoothed_train_losses = exponential_moving_average(train_losses, alpha=0.01)\n",
        "smoothed_eval_losses = exponential_moving_average(eval_losses, alpha=0.01)\n",
        "\n",
        "plt.plot(smoothed_train_losses, label='Loss liss√©e (EMA)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Courbe de la perte pendant l\\'entra√Ænement (liss√©e)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(smoothed_eval_losses, label='Loss de validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Courbe de la perte de validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# save\n",
        "\n",
        "if not path.exists(\"models\"):\n",
        "  makedirs(\"models\")\n",
        "torch.save(decoder, f\"models/password_rnn_{int(time.time())}.pt\")\n",
        "\n",
        "print(\"nG√©n√©ration de mots de passe apr√®s entra√Ænement :n\")\n",
        "for _ in range(10):\n",
        "  print(evaluate(prime_str=random.choice(selected), temperature=0.7))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le mod√®le entra√Æn√©\n",
        "if path.exists(\"models/password_lstm_attention.pt\"):\n",
        "    decoder = torch.load(\"models/password_lstm_attention.pt\").to(device)\n",
        "    decoder.eval()  # Mettre le mod√®le en mode √©valuation\n",
        "    print(\"‚úÖ Mod√®le charg√© avec succ√®s !\")\n",
        "else:\n",
        "    print(\"‚ùå Erreur : mod√®le introuvable.\")\n",
        "    exit()\n",
        "\n",
        "# Charger les donn√©es de validation\n",
        "try:\n",
        "    val_file = unidecode(open(\"sample_data/validation2.txt\").read())\n",
        "    val_file_len = len(val_file)\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Erreur : Le fichier validation2.txt est introuvable.\")\n",
        "    exit()\n",
        "\n",
        "# Fonction pour obtenir un batch de validation\n",
        "def validation_set():\n",
        "    start_index = random.randint(0, val_file_len - chunk_len - 1)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    chunk = val_file[start_index:end_index]\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target\n",
        "\n",
        "# Fonction d'√©valuation\n",
        "def evaluate_model(num_samples=500):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_losses = []\n",
        "\n",
        "    with torch.no_grad():  # D√©sactiver le calcul des gradients\n",
        "        for _ in range(num_samples):  # Tester sur plusieurs √©chantillons\n",
        "            inp, target = validation_set()\n",
        "            hidden = decoder.init_hidden()\n",
        "            loss = 0\n",
        "            seq_len = inp.size(0)\n",
        "\n",
        "            for c in range(seq_len):\n",
        "                output, hidden = decoder(inp[c].unsqueeze(0), hidden)\n",
        "                loss += criterion(output, target[c].unsqueeze(0))\n",
        "\n",
        "                # Calcul de l'accuracy\n",
        "                pred = output.argmax(dim=1)  # Trouver l'indice de la meilleure pr√©diction\n",
        "                correct += (pred == target[c]).sum().item()\n",
        "                total += 1\n",
        "\n",
        "            loss = loss.item() / seq_len\n",
        "            total_loss += loss\n",
        "            all_losses.append(loss)\n",
        "\n",
        "    avg_loss = total_loss / num_samples  # Moyenne des pertes\n",
        "    accuracy = correct / total * 100  # Pr√©cision en %\n",
        "\n",
        "    return avg_loss, accuracy, all_losses\n",
        "\n",
        "# Ex√©cuter l'√©valuation\n",
        "val_loss, val_accuracy, all_losses = evaluate_model(num_samples=500)\n",
        "\n",
        "print(f\"\\nüìä R√©sultats de la validation sur 500 √©chantillons :\")\n",
        "print(f\"   - Validation Loss : {val_loss:.4f}\")\n",
        "print(f\"   - Validation Accuracy : {val_accuracy:.2f}%\")\n",
        "\n",
        "# Affichage des r√©sultats sous forme graphique\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Graphique de la distribution des pertes\n",
        "plt.hist(all_losses, bins=30, color=\"blue\", alpha=0.7)\n",
        "plt.xlabel(\"Loss par s√©quence\")\n",
        "plt.ylabel(\"Nombre d'occurrences\")\n",
        "plt.title(\"Distribution des pertes sur l'ensemble de validation\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RbNGb3Lu6X7k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}