{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD9V/Z/TArJZL9jxtODt2R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olfabre/amsProjetSemestre2/blob/main/01_05_mod%C3%A8le.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zN7JrQNuq4k",
        "outputId": "491c025d-aa69-4ddc-bf9f-1ef5dba861b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Entraînement sur le dataset de mots de passe...\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import time, math\n",
        "import matplotlib.pyplot as plt\n",
        "from os import path, makedirs\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "all_characters = string.ascii_letters + string.digits + string.punctuation\n",
        "end_char = '&'\n",
        "selected = string.ascii_letters + string.digits + end_char\n",
        "n_characters = len(all_characters)\n",
        "n_characters_selected = len(selected)\n",
        "\n",
        "chunk_len = 12\n",
        "\n",
        "hidden_size = 256\n",
        "n_layers = 2\n",
        "lr = 0.005\n",
        "n_epochs = 3000\n",
        "\n",
        "\n",
        "train_file = unidecode.unidecode(open(\"sample_data/train2_with_&.txt\").read())\n",
        "train_file_len = len(train_file)\n",
        "val_file = unidecode.unidecode(open(\"sample_data/validation2.txt\").read())\n",
        "val_file_len = len(val_file)\n",
        "\n",
        "def random_chunk(file, file_len):\n",
        "  start_index = random.randint(0, file_len - chunk_len - 1)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "    if string[c] not in selected:\n",
        "      continue\n",
        "    tensor[c] = selected.index(string[c])\n",
        "  return tensor.to(device)\n",
        "\n",
        "def random_training_set(file, file_len):\n",
        "  chunk = random_chunk(file, file_len)\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "def train(inp, target):\n",
        "  hidden = decoder.init_hidden()\n",
        "  decoder.zero_grad()\n",
        "  loss = 0\n",
        "  seq_len = inp.size(0) # Récupérer la longueur du batch actuel\n",
        "  for c in range(seq_len):\n",
        "    output, hidden = decoder(inp[c], hidden)\n",
        "    loss += criterion(output, target[c].unsqueeze(0))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  train_losses.append(loss.item() / seq_len)\n",
        "  return loss.item() / seq_len\n",
        "\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=25, temperature=0.8):\n",
        "  if predict_len is None:\n",
        "    predict_len = random.randint(5, 12)\n",
        "\n",
        "  hidden = decoder.init_hidden()\n",
        "  prime_input = char_tensor(prime_str)\n",
        "  predicted = prime_str\n",
        "\n",
        "  for p in range(len(prime_str) - 1):\n",
        "    _, hidden = decoder(prime_input[p], hidden)\n",
        "  inp = prime_input[-1]\n",
        "\n",
        "  for _ in range(predict_len):\n",
        "    output, hidden = decoder(inp, hidden)\n",
        "    output_dist = torch.softmax(output.data.view(-1) / temperature, dim=0)\n",
        "    top_i = torch.multinomial(output_dist[:len(selected)], 1)[0]\n",
        "    predicted_char = selected[top_i]\n",
        "\n",
        "    if predicted_char == '&':\n",
        "      break\n",
        "\n",
        "    predicted += predicted_char\n",
        "    inp = torch.tensor([selected.index(predicted_char)]).to(device)\n",
        "\n",
        "  return predicted\n",
        "\n",
        "\n",
        "def time_since(since):\n",
        "  s = time.time() - since\n",
        "  m = math.floor(s / 60)\n",
        "  s -= m * 60\n",
        "  return '%dm %ds' % (m, s)\n",
        "\n",
        "def exponential_moving_average(values, alpha=0.01):\n",
        "  ema = []\n",
        "  avg = values[0] # Initialisation\n",
        "  for value in values:\n",
        "    avg = alpha * value + (1 - alpha) * avg\n",
        "    ema.append(avg)\n",
        "  return ema\n",
        "\n",
        "def evaluate_loss(n_samples=1000):\n",
        "  total_loss = 0\n",
        "  for _ in range(n_samples):\n",
        "    inp, target = random_training_set(val_file, val_file_len)\n",
        "    hidden = decoder.init_hidden()\n",
        "    loss = 0\n",
        "  for c in range(inp.size(0)):\n",
        "    output, hidden = decoder(inp[c], hidden)\n",
        "  loss += criterion(output, target[c].unsqueeze(0))\n",
        "  total_loss += loss.item() / inp.size(0)\n",
        "  return total_loss / n_samples\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers).to(device)\n",
        "optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "print(\"Entraînement sur le dataset de mots de passe...\")\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss = train(*random_training_set(train_file, train_file_len))\n",
        "  with torch.no_grad():\n",
        "    eval_loss = evaluate_loss(n_samples=50)\n",
        "    eval_losses.append(eval_loss)\n",
        "  if epoch % 1000 == 0:\n",
        "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "\n",
        "smoothed_train_losses = exponential_moving_average(train_losses, alpha=0.01)\n",
        "smoothed_eval_losses = exponential_moving_average(eval_losses, alpha=0.01)\n",
        "\n",
        "plt.plot(smoothed_train_losses, label='Loss lissée (EMA)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Courbe de la perte pendant l\\'entraînement (lissée)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(smoothed_eval_losses, label='Loss de validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Courbe de la perte de validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# save\n",
        "\n",
        "if not path.exists(\"models\"):\n",
        "  makedirs(\"models\")\n",
        "torch.save(decoder, f\"models/password_rnn_{int(time.time())}.pt\")\n",
        "\n",
        "print(\"nGénération de mots de passe après entraînement :n\")\n",
        "for _ in range(10):\n",
        "  print(evaluate(prime_str=random.choice(selected), temperature=0.7))\n",
        "\n"
      ]
    }
  ]
}